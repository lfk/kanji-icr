% Document title
% ==============
% Draft for conference/workshop paper to be submitted to http://tappcs.blogspot.mx/
% 2--4 pages in Spanish or English.

\documentclass[10pt,conference,a4paper]{IEEEtran}
\usepackage[utf8x]{inputenc}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage[T1]{fontenc}
\bibliographystyle{IEEEtran}

% This enables the kanji in the table, but all typefaces turn uglier.
% Also must be generated with xelatex
%\usepackage{xeCJK}
%\setCJKmainfont{cyberbit.ttf}


\title{Some Nice Title Goes Here, Eventually\ldots}

\author{
	\IEEEauthorblockN{Lars Fredrik Karlstr\"om}
	\IEEEauthorblockA{Faculty of Science, Dept. of Computer Science\\ Universidad Aut\'onoma de Baja California\\ \href{mailto:fredrik.karlstrm@uabc.edu.mx}{\texttt{fredrik.karlstrm@uabc.edu.mx}}}
	\and
	\IEEEauthorblockN{Everardo Guti\'errez L\'opez}
	\IEEEauthorblockA{Faculty of Science, Dept. of Computer Science\\ Universidad Aut\'onoma de Baja California\\ \href{mailto:everardo.gutierrez@uabc.edu.mx}{\texttt{everardo.gutierrez@uabc.edu.mx}}}
}

\pgfplotsset{
	compat = newest,
	xlabel near ticks,
	ylabel near ticks
}

\begin{document}
	\maketitle

	\begin{abstract}
		Quick content summary.
		Motivation. (From 'Robust': High accuracy has been attained, but speedup/size improvements still important for mobile and desktop.)
	\end{abstract}
	\medskip
	\begin{IEEEkeywords}
		Intelligent character recognition, artificial neural network, handwriting recognition, Japanese writing, \ldots
	\end{IEEEkeywords}

	\section{Introduction}
	\label{sec:introduction}

	%Although new technologies aimed at digitalizing every aspect of daily life are introduced and adopted at an ever--increasing pace,
	%handwriting still remains an integral 

	Although a variety of efficient digital input methods have become all but second nature to recent generations, for the vast majority,
	handwriting remains an integral activity in daily life.
	The importance to further improve upon the state of the art in handwriting recognition is underlined not only by the continued prevalence of 
	handwriting, but also by the need to digitalize archived material, the significant role of handwriting in the development of fine motor skills,
	and a variety of other areas of application, such as signature verification. \cite{plamondon2000online}

	As an area of research, handwriting recognition predates even the digital computer. In 1888, Elisha Gray was awarded a patent \cite{gray1888telautograph}
	for the ``Telautograph,'' which is considered a precursor to the stylus and digitizer.
	In \cite{dimond1957devices} (1957), T.L. Dimond summarized early character recognition efforts and introduced the Stylator, another input recognition device.
	
	The decades of research since has introduced a vast number of recognition techniques for writing in any and all forms; notable successful techniques in
	widespread use today include support vector machines, hidden Markov models, and artificial neural networks. \cite{fujisawa2008forty, tappert1990state}

	This paper documents a work in progress on the topic intelligent handwriting recognition of Japanese characters.
	The remainder of the article is structured as follows:
	In section \ref{sec:problem_description} we present the Japanese writing system and outline the primary issues with handwriting recognition for asian script;
	thereafter in \ref{sec:previous_work}, we discuss some notable advances aimed at solving various aspects of the aforementioned problems.
	An explanation of the proposed solution is given in section \ref{sec:proposed_solution}, and an initial, experimental implementation is described in \ref{sec:experiments}
	-- the results of which are recorded and examined in section \ref{sec:experiment_results}. Finally, conclusions and future work is presented in sections
	\ref{sec:conclusions} and \ref{sec:future_work}, respectively.

	
	\begin{figure}
		\centering
		\includegraphics[width=2.75in]{./fig/yama-ryuu-kane.eps}
		\caption{Sample characters. The kanji ``yama'' (mountain), ``ry\=u'' (dragon), and ``kane'' (bell), written with 3, 10, and 20 strokes, respectively.}
		\label{fig_kanji_sample}
	\end{figure}


	\section{Problem description}
	\label{sec:problem_description}

	Any form of digital handwriting recognition is a complex undertaking,
	where the performance of the system is impacted by a multitude of factors -- spanning
	everything from the quality and quantity of training data, to resilience toward input
	translation, rotation, and noise. As the number of candidate categories increases
	the size of the search space, however, the task quickly grows monumental.

	The Japanese writing system consists of the two \emph{kana} syllabaries \emph{hiragana} and \emph{katakana},
	as well as the sinographs commonly known as \emph{kanji} -- literally ``Han characters'' -- that stem from China.
	The Japanese Ministry of Education's official ``regular use'' (j\=oy\=o) kanji list defines a set of 2,136
	baseline characters taught in elementary-- and secondary--school education. \cite{hadamitzky2012japanese}
	This set is far from exhaustive, though: the Japanese Industrial Standard X 0208 encoding, for instance, contains 6,355 different kanji.

	Given that the category search space for Japanese characters is an order of magnitude larger than its western counterparts,
	and that other characteristics must be taken into account, it becomes apparent that the same designs that
	yields highly accurate results on the latin alphabet will not perform satisfactory when applied to asian character recognition \cite{tappert1990state, liu2004online}
	-- unless it relies on complex, computationally expensive techniques such as for instance ``deep'' neural networks. \cite{ciresan2012multi}

	In continuation, we describe a flexble, modular design scheme that splits the identification task into coarse and fine steps,
	thereby reducing the complexity of each undertaking. The design scheme also permits replacing classification strategies,
	and facilitates the parallelization of network training.




	 
	\section{Previous work}
	\label{sec:previous_work}

	Testing a cite \cite{zhu2014robust}.

	\ldots

	In \cite{nakai2001substroke}, the authors adapt a continuous speech recognition algorithm utilizing
	hidden Markov models for the purpose of online kanji recognition. By identifying and classifying
	substrokes, which in turn are employed as search keys in a character dictionary, this approach vastly
	reduced the memory requirements for both their models and their dictionary.

	
	In \cite{yang2003accelerating}, clustering, offline recognition, first comparing input to centroid character,
	therafter search within.


	``Online Handwritten Kanji Recognition Based on Inter-stroke Grammar'' uses the A-H0-9 classification scheme,
	and discusses the hierarchical structure of kanji.

	``A linear--time elastic matching algorithm for on--line recognition of handwritten Japanese characters'' (Nakagawa)
	presents the LTM--algorithm, an alternative and attractive way of extracting feature points.

	Should we mention alternative solutions, such as SVMs? Zinnia (w/ Tomoe) does quite a good job, for instance. Not sure if there
	are academic evaluations/references, though.



	\section{Proposed solution}
	\label{sec:proposed_solution}

	\begin{figure}
		\centering
		\includegraphics[width=2.75in]{./fig/model-overview.eps}
		\caption{The model consists of four components: a feature extractor, a clustering strategy,
			a router (coarse classifier), and a fine--grained classifier.}
		\label{fig_model_overview}
	\end{figure}



	By structuring the search space into mutually exclusive clusters of similar categories,
	we are able to construct a corresponding number of fine classifiers that need only distinguish
	between the categories within its own cluster. The smaller search space directly translates into a
	significant reduction of model complexity, which in turn implies both a decrease in training time
	and the possibility of higher accuracy rates. Furthermore, given that no category overlap exists
	between the clusters, each fine classifier may be trained in parallel, and each model may be gradually
	improved as new training data is made available.




	\subsection{Feature extraction}

	The input data is processed by an algorithm, generating a coarse, n--dimensional representation of its principal features.
	The feature vector is used to select which cluster the input belongs to.
	

	A higher feature vector dimensionality allows for the construction of more finely--tuned clusters, but may well increase
	the likelihood of ``misrouting,'' resulting in a failed classification.
	% Actually, if a character is not found within the specified CANN, it could be passed to a neighboring cluster of similar composition

	The coarse classifier may work on either online or offline input, or potentially a union of them both.
	Examples of feature extractors include the LTM algorithm, and gradient--based solutions, both outlined in \cite{tanaka1999hybrid}.



	\subsection{Category clustering}

	As is proven in \cite{drineas2004clustering}, this is an NP-hard problem even in the case of $k = 2$. 

	Centroid-based clustering
	"Seeks to minimize the average squared distance between points in the same cluster"
	goal is to choose k centers so as to minimize $\varphi$, the total squared distance between each point and its closest center.

	\subsection{Feedforward routing network}


	\subsection{Convolutional character classification network}


	\section{Experiments}
	\label{sec:experiments}

	This will be a short section\ldots
	Describe the simple online feature extractor and following clustering scheme.

	\begin{figure}
		\centering
		\includegraphics[width=2.75in]{./fig/experimental-implementation.eps}
		\caption{Experimental implementation. The input is first analyzed by the coarse classifier, resulting in the $<GHA\ldots>$ feature representation.
		Thereafter, the routing network selects which cluster to search in, and the image data is passed to the corresponding CANN for fine--grain classification.}
		\label{fig_experimental_implementation}
	\end{figure}



	\subsection{Data sources}

	KanjiVG \ldots


	\subsection{Feature extraction}

	Figure of A-H compass. Explanation.

	\begin{table}
		\renewcommand{\arraystretch}{1.3}
		\caption{Stroke type frequency distribution overview }
		\label{tbl_stroke_analysis}
		\centering
		\begin{tabular}{ c | c c c c c l }
			\hline
			  & \bfseries \% & \small $\sum$ & $\mu$ & $\sigma$ & Highest count \\ 
			\hline
			\hline
			\bfseries A & $33.20\%$ & $26,160$ & $4.09$ & $2.33$ & $15$ & \\%(蠶) \\
			\bfseries B & $1.47\%$  & $1,159$  & $0.18$ & $0.40$ & $2$  & \\%(摑) \\
			\bfseries C & $0.00\%$  & $0$      & --     & --     & --   & \\%     \\
			\bfseries D & $0.00\%$  & $0$      & --     & --     & --   & \\%     \\
			\bfseries E & $0.31\%$  & $245$    & $0.04$ & $0.19$ & $2$  & \\%(霪) \\
			\bfseries F & $12.72\%$ & $10,018$ & $1.57$ & $1.36$ & $10$ & \\%(鏐) \\
			\bfseries G & $29.98\%$ & $23,616$ & $3.69$ & $1.96$ & $13$ & \\%(鱸) \\
			\bfseries H & $22.32\%$ & $17,587$ & $2.75$ & $1.61$ & $11$ & \\%(靆) \\
			\hline
			            & $100.00\%$   & $78,785$ &        &        &      & \\
			\hline
		\end{tabular}
	\end{table}



	\subsection{Clustering}

	Whereas in \cite{nakai2001substroke} strokes were segmented into substrokes and classified in
	25 dimensions, our initial feature extraction strategy provides a coarse full--stroke classification
	in 8 dimensions. By discarding ``pen--up motions,'' we render the system more resilient to erroneous
	stroke order, at the cost of increasing variability within each cluster.


	\ldots decided to consider the value given by the ``rule of thumb'' $k \approx \sqrt{n / 2}$ proposed in \cite{mardia2005multivariate}
	as an upper limit to the number of clusters.

	$$ k_{max} = \sqrt{6,396 / 2} \approx 57 $$

	The clusters were generated using the k--means++ algorithm,
	a variation of Lloyd's algorithm that seeks to improve the selection of the initial centroids. \cite{arthur2007k}

	Based on the information summarized in table \ref{tbl_stroke_analysis}, \ldots

	Total strokes: 78785

	\begin{table}
		\renewcommand{\arraystretch}{1.3}
		\caption{Some form of cluster sample}
		\label{tbl_sample_clusters}
		\centering
		\begin{tabular}{c}
			We could list centroid info and some sample kanji. \\
			But not \emph{all} the clusters, right? Too many\ldots \\
		\end{tabular}
	\end{table}




	%\begin{tikzpicture}[font=\small]
	%	\begin{axis}[
	%		ybar,
	%		bar width=20pt,
	%		xlabel={Stroke feature classification},
	%		ylabel={Frequency (\%)},
	%		ymin=0,
	%		ymax=40,
	%		%ytick=\empty,
	%		xtick=data,
	%		axis x line=bottom,
	%		axis y line=left,
	%		enlarge x limits=0.2,
	%		symbolic x coords={A, B, C, D, E, F, G, H},
	%		xticklabel style={anchor=base, yshift=-\baselineskip},
	%		nodes near coords={\pgfmathprintnumber\pgfplotspointmeta}
	%	]
	%		\addplot[fill=white] coordinates {
	%			(A, 33.2)
	%			(B, 1.5)
	%			(C, 0)
	%			(D, 0)
	%			(E, 0.3)
	%			(F, 12.7)
	%			(G, 30.0)
	%			(H, 22.3)
	%		};
	%	\end{axis}
	%\end{tikzpicture}

	\section{Preliminary results}
	\label{sec:experiment_results}

	Not done yet, but here we go.


	\section{Conclusions}
	\label{sec:conclusions}

	This work is cool, now give complimentary M.Sc. plx?
	They did it for Donald Knuth\ldots :(


	\section{Future work}
	\label{sec:future_work}

	The experimental results documented in this paper are an initial attempt at applying the proposed methodology;
	numerous variations remain to be explored. With regard to online recognition, the explored feature extraction
	strategy could be enhanced to break strokes into substrokes, diffentiate between long and short strokes, and take
	pen--up movements into account. This alteration would in turn allow clustering on up to 25 dimensions, rather than the current 8.

	By substituting the current coarse classifier with an offline feature extraction strategy, the entire system could
	be evaluated on offline handwriting recognition datasets, such as\ldots. 

	Clustering\ldots

	Finally, at the cluster--specific CANN--level, there is ample opportunity to tweak the network topology as well as
	to apply a variety of best practices \cite{simard2003best} in order to enhance the final, fine--grain recognition results.



	% References
	\bibliography{./references}

\end{document}
